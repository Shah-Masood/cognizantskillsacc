---------------------PART 1: THEORY OF FINE-TUNING---------------------


Concept Check:

What is the main benefit of fine-tuning an LLM?

A) It improves the model’s speed.
B) It customizes the model for specific tasks or domains.
C) It eliminates the need for high-quality datasets.
D) It prevents overfitting entirely.
(Correct Answer: B)
Which of the following describes "catastrophic forgetting"?

A) When the model forgets its pre-training data during inference.
B) When the model loses its generalization ability after excessive fine-tuning on a specific task.
C) When the model produces irrelevant outputs due to overfitting.
D) When the model fails to save fine-tuned weights.
(Correct Answer: B)

Application Task:

Transfer learning is like learning a second language after mastering the first. Imagine you’re fluent in Spanish, 
and now you want to learn Italian. Since both languages share similar grammar rules and vocabulary, you don’t have to start from scratch. Instead, you transfer your existing knowledge to learn Italian faster.
In machine learning, transfer learning follows the same principle. A model trained on a large dataset (e.g., ImageNet for image classification) can be fine-tuned on a smaller, 
domain-specific dataset (e.g., X-ray images for pneumonia detection). Instead of training from the ground up, the model uses pre-learned patterns and adapts them to the new task, significantly reducing training time and data requirements.

Review_ID	        Review_Text	                                Sentiment
101	    "This movie was fantastic! The storyline was gripping."	Positive
102	    "Terrible acting, and the plot made no sense."	        Negative
103	    "Decent film, but not worth watching again."	        Neutral
104	    "Absolutely loved it! Best film of the year."	        Positive
105	    "The worst movie I've seen in a long time."	            Negative

Cleaning Steps:
Remove stopwords (e.g., the, is, and).
Normalize text (convert to lowercase).
Remove punctuation and special characters.
Tokenize and lemmatize for better model performance.


---------------------PART 2: PRACTICAL FINE-TUNING SESSION---------------------


Reflection:

ine-tuning DistilBERT for sentiment classification on the IMDB dataset presented several challenges, primarily related to memory 
constraints, training efficiency, and model generalization. Handling a large dataset required careful batch size management to prevent 
out-of-memory (OOM) errors, which were mitigated by reducing the per-device batch size to 8 and enabling gradient accumulation when necessary. Training time was another concern, 
as fine-tuning a transformer model on large text data can be slow. To optimize performance, I incorporated mixed precision training (fp16=True) and leveraged data prefetching to speed up processing. 
Overfitting was also a risk, especially with a small number of training epochs, so weight decay (0.01) and early stopping were implemented to improve generalization.

If the model’s accuracy remained below 90%, several optimizations could be made. Hyperparameter tuning, such as adjusting the learning rate (3e-5 or 5e-5) or increasing the batch size, 
could improve model performance. Enhancing data preprocessing through stopword removal, lemmatization, or data augmentation (e.g., synonym replacement or back translation) could make the model more robust. Additionally, switching to a more powerful model like 
BERT-base or RoBERTa-base could improve results, albeit at the cost of higher compute requirements. Training on additional sentiment datasets, such as Amazon reviews or Yelp data, could also enhance generalization, ensuring the model performs well across diverse domains.